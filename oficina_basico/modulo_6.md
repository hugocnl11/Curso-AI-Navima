# 🛡️ Módulo 6: Ética y privacidad

El uso de herramientas de inteligencia artificial plantea importantes cuestiones éticas y de protección de datos.  
Este módulo te ayudará a comprender los **riesgos**, las **responsabilidades** y las **buenas prácticas** relacionadas con la privacidad y el uso ético de la IA en el entorno laboral.

---

## Riesgos de privacidad y confidencialidad al usar IA

Las IA como ChatGPT procesan texto, pero no tienen memoria privada dentro de la empresa.  
Esto significa que:

- No debes compartir **datos sensibles**: nombres completos de clientes, DNI, cuentas bancarias, contraseñas, etc.
- Todo lo que se introduce en una IA pública puede ser almacenado y analizado por el proveedor, aunque sea de forma anónima.
- Algunas plataformas permiten entrenar modelos internos, pero requieren configuraciones específicas de seguridad.

**Ejemplo de mal uso:**
> "ChatGPT, redacta este contrato para el cliente María López con NIF 98765432X y dirección Calle Mayor 25..."

---

## Cómo proteger información sensible y cumplir normativas

1. **Evita introducir datos personales o confidenciales** en herramientas abiertas.
2. **Consulta con IT o Legal** si necesitas usar IA para datos reales de la empresa.
3. **Utiliza IA local o privada** cuando sea posible (por ejemplo, Microsoft Copilot bajo políticas internas).
4. **Avisa siempre** si un contenido ha sido generado con IA (transparencia).
5. **Cumple el RGPD y otras normativas** según tu rol y los datos que manejes.

### Herramientas con enfoque seguro:
- Microsoft Copilot (M365)
- ChatGPT Team / Enterprise
- Plataformas con control de datos (locales o cloud privados)

---

## Límites éticos del uso de la IA en contextos laborales

Más allá de lo legal, hay límites que conviene respetar en cuanto a:

- **Representación honesta del trabajo**: no presentar como propio algo 100% generado por IA sin revisión.
- **Evitar sesgos**: la IA puede reproducir estereotipos o discriminaciones si no se revisa.
- **No depender ciegamente de la tecnología**: siempre debe haber supervisión humana.
- **Uso transparente**: si colaboras con IA, dilo cuando sea relevante.

**Ejemplo positivo:**
> "Este informe ha sido generado con la ayuda de una herramienta de IA y revisado por el equipo de contabilidad."

---

## Actividad práctica

> Reflexiona sobre cómo usas actualmente herramientas de IA:  
> - ¿Has introducido alguna vez información sensible sin darte cuenta?  
> - ¿Cuándo consideras que sería poco ético usar una IA?  
> - ¿Cómo puedes mejorar tu uso responsable a partir de ahora?

Escribe tus respuestas y compártelas en grupo si lo deseas.

---

## Recursos adicionales

- [Guía de privacidad y seguridad en IA (Agencia Española de Protección de Datos)](https://www.aepd.es/)
- [Principios éticos para la IA (Comisión Europea)](https://digital-strategy.ec.europa.eu/)
- [Normativa sobre uso de IA en el entorno laboral (PDF interno)](/oficina_basico/stuff/etica_privacidad_navima.pdf)

---

<p align="center">
  <a href="https://hugocnl11.github.io/Formacion-interna-Navima/oficina_basico/modulo_5.html">⏮️ Módulo anterior</a> 
  <a href="https://hugocnl11.github.io/Formacion-interna-Navima/curso_ia_oficina.html">Descripción del curso 📚</a>
</p>
